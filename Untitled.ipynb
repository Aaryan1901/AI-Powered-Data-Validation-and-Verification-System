{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa186e7-8e83-4ed0-9934-666941cd9df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Excel file.\n",
      "Loading prompt from text file.\n",
      "Starting parallel processing of rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|█████████████████████████████████████████████████████████████████| 63/63 [00:46<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing all rows\n",
      "Verifying model results and updating 'Result_Check' column.\n",
      "Total rows processed: 63\n",
      "Correct matches: 17\n",
      "Rows where Result_Model1 is wrong: 31\n",
      "Rows where Result_Model2 is wrong: 36\n",
      "Rows where model comparison was not needed: 8\n",
      "Saving the results to Excel.\n",
      "Processing completed and file saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the LLM Foundry token for your API requests\n",
    "LLMFOUNDRY_TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6Im11dGh1a3VtYXIucGFuY2hhYmVrYXNhbkBzdHJhaXZlLmNvbSJ9.uiwWDBAUFxkHaLY4duukUT0h94izwJH6rktK5mksef0\"\n",
    "\n",
    "# Function to interact with the first model\n",
    "def chat_with_llm_model1(user_input):\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"https://llmfoundry.straive.com/azure/openai/deployments/gpt-35-turbo/chat/completions?api-version=2024-05-01-preview\",\n",
    "        headers={\"Authorization\": f\"Bearer {LLMFOUNDRY_TOKEN}:Muthu_bot\"},\n",
    "        json={\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        answer = response_json['choices'][0]['message']['content']\n",
    "        return answer\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Function to interact with the second model\n",
    "def chat_with_llm_model2(user_input):\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"https://llmfoundry.straive.com/anthropic/v1/messages\",\n",
    "        headers={\"Authorization\": f\"Bearer {LLMFOUNDRY_TOKEN}:Muthu_bot\"},\n",
    "        json={\"model\": \"claude-3-haiku-20240307\", \"max_tokens\": 1000, \"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        answer = response_json['content'][0]['text']\n",
    "        return answer\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "# Load the Excel file and the specific sheet\n",
    "file_path = r'C:\\Users\\Aaryan\\Documents\\project-1_materials-20250512T132232Z-1-001\\project-1_materials\\city_name.xlsx'\n",
    "sheet_name = 'Cosmo Input Report'\n",
    "print(\"Loading Excel file.\")\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Load the prompt from the text file\n",
    "print(\"Loading prompt from text file.\")\n",
    "with open(r'C:\\Users\\Aaryan\\Documents\\project-1_materials-20250512T132232Z-1-001\\project-1_materials\\prompt.txt', 'r') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# Check for result columns and add them if they don't exist\n",
    "for col in ['Result_Model1', 'Result_Model2', 'City_Check']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "        \n",
    "MAX_THREADS = 5\n",
    "matches = 0\n",
    "# Function to process each row and update results for both models\n",
    "def process_row(index, row):\n",
    "    main_city = row['Source City name']\n",
    "    description = row['Description']\n",
    "    prompt = prompt_template.format(main_city=main_city, description=description)\n",
    "\n",
    "    result_model1 = chat_with_llm_model1(prompt)\n",
    "    result_model2 = chat_with_llm_model2(prompt)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    city_name_lower = main_city.lower()\n",
    "    description_lower = description.lower()\n",
    "    city_check = (city_name_lower in description_lower) or (f\"{city_name_lower}'s\" in description_lower)\n",
    "    \n",
    "    city_check_result = \"City name available and matching\" if city_check else \"City name not available or mismatched\"\n",
    "    \n",
    "    return index, result_model1, result_model2, city_check_result\n",
    "\n",
    "# Parallel processing of the rows\n",
    "total_rows = len(df)\n",
    "print(\"Starting parallel processing of rows.\")\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    futures = {executor.submit(process_row, i, row): i for i, row in df.iterrows()}\n",
    "    for future in tqdm(futures, total=total_rows, desc=\"Processing Rows\"):\n",
    "        i, result_model1, result_model2, city_check_result = future.result()\n",
    "        df.at[i, 'Result_Model1'] = result_model1\n",
    "        df.at[i, 'Result_Model2'] = result_model2\n",
    "        df.at[i, 'City_Check'] = city_check_result\n",
    "    print(\"finished processing all rows\")\n",
    "\n",
    "def verify_model_results(row):\n",
    "    expected_result = \"a. City name available and main city matching\"\n",
    "    city_check_phrase = \"City name available and matching\"\n",
    "    model1_status = \"Correct\" if row['Result_Model1'] == expected_result else \"Result_Model1 is wrong\"\n",
    "    model2_status = \"Correct\" if row['Result_Model2'] == expected_result else \"Result_Model2 is wrong\"\n",
    "    overall_status = \"\"\n",
    "\n",
    "    if row['City_Check'] == city_check_phrase:\n",
    "        if row['Result_Model1'] != expected_result:\n",
    "            overall_status += model1_status\n",
    "        if row['Result_Model2'] != expected_result:\n",
    "            overall_status += \" and \" + model2_status if overall_status else model2_status\n",
    "    else:\n",
    "        # If City_Check is not as expected, no comparison is required with model results.\n",
    "        overall_status = \"City not matching, no need to check models.\"\n",
    "    \n",
    "    return overall_status if overall_status else \"Correct\"\n",
    "\n",
    "# Ensure the fourth column exists\n",
    "if 'Result_Check' not in df.columns:\n",
    "    df['Result_Check'] = None\n",
    "\n",
    "print(\"Verifying model results and updating 'Result_Check' column.\")\n",
    "df['Result_Check'] = df.apply(verify_model_results, axis=1)\n",
    "\n",
    "# Calculate summary information\n",
    "correct_count = df['Result_Check'].value_counts().get(\"Correct\", 0)\n",
    "wrong_model1_count = df['Result_Check'].str.contains(\"Result_Model1 is wrong\").sum()\n",
    "wrong_model2_count = df['Result_Check'].str.contains(\"Result_Model2 is wrong\").sum()\n",
    "no_check_needed_count = df['Result_Check'].str.contains(\"City not matching\").sum()\n",
    "\n",
    "# Output summary information\n",
    "print(f\"Total rows processed: {len(df)}\")\n",
    "print(f\"Correct matches: {correct_count}\")\n",
    "print(f\"Rows where Result_Model1 is wrong: {wrong_model1_count}\")\n",
    "print(f\"Rows where Result_Model2 is wrong: {wrong_model2_count}\")\n",
    "print(f\"Rows where model comparison was not needed: {no_check_needed_count}\")\n",
    "\n",
    "# Save the modified DataFrame back to Excel\n",
    "print(\"Saving the results to Excel.\")\n",
    "df.to_excel(r'C:\\Users\\Aaryan\\Documents\\project-1_materials-20250512T132232Z-1-001\\project-1_materials\\processed_city_name.xlsx', sheet_name=sheet_name, index=False)\n",
    "print(\"Processing completed and file saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
